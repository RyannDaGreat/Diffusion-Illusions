{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flippy Illusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX4qkH_B_LWe"
   },
   "source": [
    "Hi! Welcome to the official colab demo for our demo \"Diffusion Illusions: Hiding Images in Plain Sight\". [https://ryanndagreat.github.io/Diffusion-Illusions/](https://ryanndagreat.github.io/Diffusion-Illusions/)\n",
    "\n",
    "This project was inspired by our paper \"Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors\". The Peekaboo project website: [https://ryanndagreat.github.io/peekaboo/](https://ryanndagreat.github.io/peekaboo/)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "0. Go to the Runtime menu, and make sure this notebook is using GPU!\n",
    "1. Run the top 2 code cells (one cleans colab's junk and downloads the source code, while the other installs python packages)\n",
    "2. Click 'Runtime', then 'Restart Runtime'. You need to do this the first time you open this notebook to avoid weird random errors from the pip installations.\n",
    "3. Run code cells to load stable diffusion. The first time you run it it will take a few minutes to download; subsequent times won't take long at all though.\n",
    "4. Run all the cells below that, and customize prompt_a and prompt_b!\n",
    "5. Take the result top_image and bottom_image, print them out, and shine a backlight through them like shown in the Diffusion Illusion website (link above!)\n",
    "\n",
    "I may also create a YouTube tutorial if there's interest. Let me know if this would be helpful!\n",
    "\n",
    "This notebook was written by Ryan Burgert. Feel free to reach out to me at rburgert@cs.stonybrook.edu if you have any questions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80riJZ7f_LyL"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# if [ ! -d \".git\" ]; then \n",
    "#     rm -rf * .*; #Get rid of Colab's default junk files\n",
    "#     git clone -b master https://github.com/RyannDaGreat/Diffusion-Illusions .\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGDndq0_bch2",
    "outputId": "81a50af6-5861-4990-dfe5-871ac1682d28"
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade -r requirements.txt\n",
    "# %pip install rp --upgrade\n",
    "# # You may need to restart the runtime after installing these\n",
    "# # I'm not sure why this helps, but all sorts of weird random errors pop up in Colab if you don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAtxvveUbquu"
   },
   "outputs": [],
   "source": [
    "from rp import *\n",
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import source.stable_diffusion as sd\n",
    "from easydict import EasyDict\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.stable_diffusion_labels import NegativeLabel\n",
    "from itertools import chain\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7A1Fw50eDjJ",
    "outputId": "37ceb472-da12-4c74-a76b-a89f39a3a14d"
   },
   "outputs": [],
   "source": [
    "#ONLY GOOD PROMPTS HERE\n",
    "example_prompts = rp.load_yaml_file('source/example_prompts.yaml')\n",
    "print('Available example prompts:', ', '.join(example_prompts))\n",
    "\n",
    "#These prompts are all strings - you can replace them with whatever you want! By default it lets you choose from example prompts\n",
    "#Here are some on the site! It does take some thought to come up with good ideas; the upside-down should look vaguely like the right-side up to work nicely\n",
    "prompt_a, prompt_b = rp.gather(example_prompts, 'victorial_dress victorial_dress'.split())\n",
    "prompt_a, prompt_b = rp.gather(example_prompts, 'pencil_giraffe_head pencil_penguin'.split())\n",
    "prompt_a, prompt_b = rp.gather(example_prompts, 'emma_watson walter_white'.split())\n",
    "prompt_a, prompt_b = rp.gather(example_prompts, 'sydney_opera_house mount_fuji'.split())\n",
    "# prompt_a, prompt_b = rp.gather(example_prompts, 'sailing_ship sailing_ship'.split())\n",
    "# prompt_a = \"A coffee mug, steamy ceramic coffee mug, 4k photography\"\n",
    "# prompt_b = \"A donut. A frosted donut with sprinkles, delicious 4k photography\"\n",
    "prompt_b = \"A donut. A shiny frosted chocolate donut. Beautiful 4k photography unreal engine 3d\"\n",
    "prompt_a = \"A ceramic coffee mug with handle viewed from side and steam rising. Beautiful 4k photography unreal engine 3d \"\n",
    "\n",
    "negative_prompt = ''\n",
    "\n",
    "print()\n",
    "print('Negative prompt:',repr(negative_prompt))\n",
    "print()\n",
    "print('Chosen prompts:')\n",
    "print('    prompt_a =', repr(prompt_a)) #This will be right-side up\n",
    "print('    prompt_b =', repr(prompt_b)) #This will be upside-down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0eh7vWFfPQ6"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "ec572d79f75648e69c18f8fbb8445abe",
      "8a95ebc987364a3cbbc27866ff8c964b",
      "13511603d504480489e9de201dd85211",
      "f72921cdc424455e8d2e86b0ea07a29b",
      "df5753e3d2b54078a5ff58f1f0f5de78",
      "f4b3425062c041b3aa5db68a73ea0a01",
      "eb7e46a3bff547ba8d07ebc87eddd9a0",
      "875b8337c4304ae0b9ea63eb7435d3ba",
      "f767f331f3af45d4a3814a5447d29004",
      "19d10e0e23fa460c8ba196c1691ecef3",
      "38124b85cb824de2a2640dfede710372"
     ]
    },
    "id": "wi9Y9Zp5ejSP",
    "outputId": "b79dad35-3726-4efb-f640-dfb3f27df788"
   },
   "outputs": [],
   "source": [
    "if 's' not in dir():\n",
    "    model_name=\"CompVis/stable-diffusion-v1-4\"\n",
    "    model_name=\"runwayml/stable-diffusion-v1-5\"\n",
    "    # model_name=\"nitrosocke/Arcane-Diffusion\"\n",
    "    gpu=rp.select_torch_device()\n",
    "    s=sd.StableDiffusion(gpu,model_name)\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL_pjdcFekG6"
   },
   "outputs": [],
   "source": [
    "label_a = NegativeLabel(prompt_a,negative_prompt)\n",
    "label_b = NegativeLabel(prompt_b,negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LoGGFTkelJJ"
   },
   "outputs": [],
   "source": [
    "#Image Parametrization and Initialization (this section takes vram)\n",
    "\n",
    "#Select Learnable Image Size (this has big VRAM implications!):\n",
    "#Note: We use implicit neural representations for better image quality\n",
    "#They're previously used in our paper \"TRITON: Neural Neural Textures make Sim2Real Consistent\" (see tritonpaper.github.io)\n",
    "# ... and that representation is based on Fourier Feature Networks (see bmild.github.io/fourfeat)\n",
    "# learnable_image_maker = lambda: LearnableImageFourier(height=256, width=256, hidden_dim=256, num_features=128).to(s.device); SIZE=256\n",
    "\n",
    "# learnable_image_maker = lambda: LearnableImageFourier(height=512,width=512,num_features=256,hidden_dim=256,scale=20).to(s.device);SIZE=512\n",
    "learnable_image_maker = lambda: LearnableImageFourier(height=300,width=300,num_features=256,hidden_dim=256,scale=20).to(s.device);SIZE=512\n",
    "\n",
    "\n",
    "\n",
    "image=learnable_image_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44f6FT72ems4"
   },
   "outputs": [],
   "source": [
    "# 2023-12-20 04:37:25.394973\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rotate_tiles(image, num_divisions=4):\n",
    "    image=as_torch_image(image)\n",
    "    # Assuming image is a tensor of shape (num_channels, height, width)\n",
    "    num_channels, height, width = image.shape\n",
    "\n",
    "    tile_size=width//num_divisions\n",
    "    \n",
    "    # Calculate the number of tiles in each dimension\n",
    "    tiles_x = width // tile_size\n",
    "    tiles_y = height // tile_size\n",
    "\n",
    "    # Initialize an output tensor\n",
    "    output = torch.zeros_like(image)\n",
    "\n",
    "    for x in range(tiles_x):\n",
    "        for y in range(tiles_y):\n",
    "            # Extract the tile\n",
    "            tile = image[:, y*tile_size:(y+1)*tile_size, x*tile_size:(x+1)*tile_size]\n",
    "\n",
    "            # Check if the tile should be rotated 90 or -90 degrees (checker pattern)\n",
    "            if (x + y) % 2 == 0:\n",
    "                # Rotate 90 degrees\n",
    "                tile = tile.rot90(1, [1, 2])\n",
    "            else:\n",
    "                # Rotate -90 degrees\n",
    "                tile = tile.rot90(-1, [1, 2])\n",
    "\n",
    "            # Place the rotated tile back in the output tensor\n",
    "            output[:, y*tile_size:(y+1)*tile_size, x*tile_size:(x+1)*tile_size] = tile\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "uv_map_a = \"UV_Maps/ParkerPuzzleSep7_UV_Label_Exr/Image0000.exr\"\n",
    "uv_map_b = \"UV_Maps/ParkerPuzzleSep7_UV_Label_Exr/Image0113.exr\"\n",
    "uv_map_a = rp.as_torch_image(rp.load_image(uv_map_a, use_cache=True)).to(device)\n",
    "uv_map_b = rp.as_torch_image(rp.load_image(uv_map_b, use_cache=True)).to(device)\n",
    "\n",
    "def apply_uv_map(texture,uv_map):\n",
    "    uv_map = rp.torch_resize_image(uv_map, size=rp.get_image_dimensions(texture))\n",
    "    x=uv_map[1,:,:]\n",
    "    y=uv_map[0,:,:]\n",
    "    x*=get_image_width(texture)\n",
    "    y*=get_image_height(texture)\n",
    "    out=torch_remap_image(texture,x,y)\n",
    "    return out\n",
    "\n",
    "rp.display_image(uv_map_a)\n",
    "rp.display_image(uv_map_b)\n",
    "\n",
    "# learnable_image_a=lambda: image() #Right-side up\n",
    "# learnable_image_b=lambda: image().rot90(k=2,dims=[1,2]) #Upside-down\n",
    "# learnable_image_b=lambda: rotate_tiles(image()) #Upside-down\n",
    "\n",
    "learnable_image_a = lambda: apply_uv_map(image(), uv_map_a)\n",
    "learnable_image_b = lambda: apply_uv_map(image(), uv_map_b)\n",
    "\n",
    "optim=torch.optim.SGD(image.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKCQQg9teoMt"
   },
   "outputs": [],
   "source": [
    "labels=[label_a,label_b]\n",
    "learnable_images=[learnable_image_a,learnable_image_b]\n",
    "\n",
    "#The weight coefficients for each prompt. For example, if we have [0,1], then only the upside-down mode will be optimized\n",
    "weights=[2,2]\n",
    "\n",
    "weights=rp.as_numpy_array(weights)\n",
    "weights=weights/weights.sum()\n",
    "weights=weights*len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWpLV_Toertv"
   },
   "outputs": [],
   "source": [
    "#For saving a timelapse\n",
    "ims=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6-FlVZPes09"
   },
   "outputs": [],
   "source": [
    "def get_display_image():\n",
    "    return rp.tiled_images(\n",
    "        [\n",
    "            rp.as_numpy_image(learnable_image_a()),\n",
    "            rp.as_numpy_image(learnable_image_b()),\n",
    "        ],\n",
    "        length=len(learnable_images),\n",
    "        border_thickness=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "bB-Uv4Y5et8J",
    "outputId": "0cf6d601-40a6-4140-f0c5-e4720d389822"
   },
   "outputs": [],
   "source": [
    "NUM_ITER=10000000\n",
    "\n",
    "#Set the minimum and maximum noise timesteps for the dream loss (aka score distillation loss)\n",
    "s.max_step=MAX_STEP=990\n",
    "s.min_step=MIN_STEP=10 \n",
    "\n",
    "television = rp.JupyterDisplayChannel()\n",
    "television.display()\n",
    "\n",
    "display_eta=rp.eta(NUM_ITER, title='Status')\n",
    "\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "print('Every %i iterations we display an image in the form [image_a, image_b], where'%DISPLAY_INTERVAL)\n",
    "print('    image_a = (the right-side up image)')\n",
    "print('    image_b = (image_a, but upside down)')\n",
    "print()\n",
    "print('Interrupt the kernel at any time to return the currently displayed image')\n",
    "print('You can run this cell again to resume training later on')\n",
    "print()\n",
    "print('Please expect this to take quite a while to get good images (especially on the slower Colab GPU\\'s)! The longer you wait the better they\\'ll be')\n",
    "\n",
    "try:\n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num) #Print the remaining time\n",
    "\n",
    "        preds=[]\n",
    "        for label,learnable_image,weight in rp.random_batch(list(zip(labels,learnable_images,weights)), batch_size=1):\n",
    "            pred=s.train_step(\n",
    "                label.embedding,\n",
    "                learnable_image()[None],\n",
    "\n",
    "                #PRESETS (uncomment one):\n",
    "                noise_coef=.1*weight,guidance_scale=100,#10\n",
    "                # noise_coef=0,image_coef=-.01,guidance_scale=50,\n",
    "                # noise_coef=0,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.010,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1*weight, image_coef=-.005*weight, guidance_scale=50,\n",
    "            )\n",
    "            preds+=list(pred)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num%(DISPLAY_INTERVAL*50):\n",
    "                #Wipe the slate every 50 displays so they don't get cut off\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "\n",
    "            if not iter_num%(DISPLAY_INTERVAL//4):\n",
    "                im = get_display_image()\n",
    "                ims.append(im)\n",
    "                television.update(im)\n",
    "                \n",
    "                if not iter_num%DISPLAY_INTERVAL:\n",
    "                    rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Interrupted early at iteration %i'%iter_num)\n",
    "    im = get_display_image()\n",
    "    ims.append(im)\n",
    "    rp.display_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwhrgJiie3mX"
   },
   "outputs": [],
   "source": [
    "print('Right-side up image:')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_a()))\n",
    "\n",
    "print('Upside-down image:')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_b()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YJJw4dXe4JJ"
   },
   "outputs": [],
   "source": [
    "def save_run(name):\n",
    "    folder=\"untracked/parker_puzzle_runs/%s\"%name\n",
    "    if rp.path_exists(folder):\n",
    "        folder+='_%i'%time.time()\n",
    "    rp.make_directory(folder)\n",
    "    ims_names=['ims_%04i.png'%i for i in range(len(ims))]\n",
    "    with rp.SetCurrentDirectoryTemporarily(folder):\n",
    "        rp.save_images(ims,ims_names,show_progress=True)\n",
    "    print()\n",
    "    print('Saved timelapse to folder:',repr(folder))\n",
    "    \n",
    "save_run('-'.join([prompt_a,prompt_b])) #You can give it a good custom name if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "pipe_im2im=StableDiffusionImg2ImgPipeline(\n",
    "    vae                     = s.pipe.vae                          ,\n",
    "    text_encoder            = s.pipe.text_encoder                 ,\n",
    "    tokenizer               = s.pipe.tokenizer                    ,\n",
    "    unet                    = s.pipe.unet                         ,\n",
    "    scheduler               = s.pipe.scheduler                    ,\n",
    "    safety_checker          = s.pipe.safety_checker               ,\n",
    "    feature_extractor       = s.pipe.feature_extractor            ,\n",
    "    requires_safety_checker = s.pipe.requires_safety_checker      ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "device = \"cuda\"\n",
    "# model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "# pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(device)\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image = init_image.resize((768, 512))\n",
    "prompt = \"A fantasy landscape, trending on artstation\"\n",
    "images = pipe_im2im(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
    "images[0].save(\"fantasy_landscape.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python diffilu",
   "language": "python",
   "name": "diffilu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
