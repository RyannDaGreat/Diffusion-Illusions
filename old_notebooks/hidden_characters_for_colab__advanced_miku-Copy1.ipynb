{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX4qkH_B_LWe"
   },
   "source": [
    "Hi! Welcome to the official colab demo for our demo \"Diffusion Illusions: Hiding Images in Plain Sight\". [https://ryanndagreat.github.io/Diffusion-Illusions/](https://ryanndagreat.github.io/Diffusion-Illusions/)\n",
    "\n",
    "This project was inspired by our paper \"Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors\". The Peekaboo project website: [https://ryanndagreat.github.io/peekaboo/](https://ryanndagreat.github.io/peekaboo/)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "0. Go to the Runtime menu, and make sure this notebook is using GPU!\n",
    "1. Run the top 2 code cells (one cleans colab's junk and downloads the source code, while the other installs python packages)\n",
    "2. Click 'Runtime', then 'Restart Runtime'. You need to do this the first time you open this notebook to avoid weird random errors from the pip installations.\n",
    "3. Run code cells to load stable diffusion. The first time you run it it will take a few minutes to download; subsequent times won't take long at all though.\n",
    "4. Run all the cells below that, and customize prompt_w, prompt_x, prompt_y, and prompt_z!\n",
    "5. Take the result top_image and bottom_image, print them out, and shine a backlight through them like shown in the Diffusion Illusion website (link above!)\n",
    "\n",
    "I may also create a YouTube tutorial if there's interest. Let me know if this would be helpful!\n",
    "\n",
    "This notebook was written by Ryan Burgert. Feel free to reach out to me at rburgert@cs.stonybrook.edu if you have any questions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAtxvveUbquu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import source.stable_diffusion as sd\n",
    "from easydict import EasyDict\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.stable_diffusion_labels import NegativeLabel\n",
    "from itertools import chain\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7A1Fw50eDjJ",
    "outputId": "37ceb472-da12-4c74-a76b-a89f39a3a14d"
   },
   "outputs": [],
   "source": [
    "#ONLY GOOD PROMPTS HERE\n",
    "example_prompts = rp.load_yaml_file('source/example_prompts.yaml')\n",
    "print('Available example prompts:', ', '.join(example_prompts))\n",
    "\n",
    "title='miku froggo lipstick kitten_in_box darth_vader'\n",
    "title='miku miku miku miku picard'\n",
    "title='froggo froggo froggo froggo porche'\n",
    "title='pencil_cow pencil_penguin pencil_dog_head pencil_giraffe_head pencil_cat_head'\n",
    "#These prompts are all strings - you can replace them with whatever you want! By default it lets you choose from example prompts\n",
    "prompt_a, prompt_b, prompt_c, prompt_d, prompt_z = rp.gather(example_prompts, title.split())\n",
    "#Prompts a,b,c,d are the normal looking images\n",
    "#Prompt z is the hidden image you get when you overlay them all on top of each other\n",
    "\n",
    "negative_prompt = ''\n",
    "\n",
    "prompt_c=\"an intricate detailed hb pencil sketch of a puppy dog bichon head\"\n",
    "SK='hb pencil sketch'\n",
    "CO='photorealistic color oil painting'\n",
    "prompt_a=prompt_a.replace(SK,CO)\n",
    "prompt_b=prompt_b.replace(SK,CO)\n",
    "prompt_c=prompt_c.replace(SK,CO)\n",
    "prompt_d=prompt_d.replace(SK,CO)\n",
    "prompt_z=prompt_z.replace(SK,CO)\n",
    "\n",
    "print()\n",
    "print('Negative prompt:',repr(negative_prompt))\n",
    "print()\n",
    "print('Chosen prompts:')\n",
    "print('    prompt_a =', repr(prompt_a))\n",
    "print('    prompt_b =', repr(prompt_b))\n",
    "print('    prompt_c =', repr(prompt_c))\n",
    "print('    prompt_d =', repr(prompt_d))\n",
    "print('    prompt_z =', repr(prompt_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0eh7vWFfPQ6"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "ec572d79f75648e69c18f8fbb8445abe",
      "8a95ebc987364a3cbbc27866ff8c964b",
      "13511603d504480489e9de201dd85211",
      "f72921cdc424455e8d2e86b0ea07a29b",
      "df5753e3d2b54078a5ff58f1f0f5de78",
      "f4b3425062c041b3aa5db68a73ea0a01",
      "eb7e46a3bff547ba8d07ebc87eddd9a0",
      "875b8337c4304ae0b9ea63eb7435d3ba",
      "f767f331f3af45d4a3814a5447d29004",
      "19d10e0e23fa460c8ba196c1691ecef3",
      "38124b85cb824de2a2640dfede710372"
     ]
    },
    "id": "wi9Y9Zp5ejSP",
    "outputId": "b79dad35-3726-4efb-f640-dfb3f27df788"
   },
   "outputs": [],
   "source": [
    "if 's' not in dir():\n",
    "    model_name=\"CompVis/stable-diffusion-v1-4\"\n",
    "    gpu='cuda:0'\n",
    "    s=sd.StableDiffusion(gpu,model_name)\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL_pjdcFekG6"
   },
   "outputs": [],
   "source": [
    "label_a = NegativeLabel(prompt_a,negative_prompt)\n",
    "label_b = NegativeLabel(prompt_b,negative_prompt)\n",
    "label_c = NegativeLabel(prompt_c,negative_prompt)\n",
    "label_d = NegativeLabel(prompt_d,negative_prompt)\n",
    "label_z = NegativeLabel(prompt_z,negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LoGGFTkelJJ"
   },
   "outputs": [],
   "source": [
    "#Image Parametrization and Initialization (this section takes vram)\n",
    "\n",
    "#Select Learnable Image Size (this has big VRAM implications!):\n",
    "#Note: We use implicit neural representations for better image quality\n",
    "#They're previously used in our paper \"TRITON: Neural Neural Textures make Sim2Real Consistent\" (see tritonpaper.github.io)\n",
    "# ... and that representation is based on Fourier Feature Networks (see bmild.github.io/fourfeat)\n",
    "learnable_image_maker = lambda: LearnableImageFourier(height=256, width=256, hidden_dim=256, num_features=128).to(s.device); SIZE=256\n",
    "learnable_image_maker = lambda: LearnableImageFourier(height=384,width=384,num_features=256,hidden_dim=256,scale=15).to(s.device);SIZE=384\n",
    "# learnable_image_maker = lambda: LearnableImageFourier(height=512,width=512,num_features=256,hidden_dim=256,scale=20).to(s.device);SIZE=512\n",
    "\n",
    "image_a=learnable_image_maker()\n",
    "image_b=learnable_image_maker()\n",
    "image_c=learnable_image_maker()\n",
    "image_d=learnable_image_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44f6FT72ems4"
   },
   "outputs": [],
   "source": [
    "CLEAN_MODE = True # If it's False, we augment the images by randomly simulating how good a random printer might be when making the overlays...\n",
    "\n",
    "def simulate_overlay(a,b,c,d):\n",
    "    if CLEAN_MODE:\n",
    "        exp=1\n",
    "        brightness=3\n",
    "        black=0\n",
    "    else:\n",
    "        exp=rp.random_float(.5,1)\n",
    "        brightness=rp.random_float(1,5)\n",
    "        black=rp.random_float(0,.5)\n",
    "        bottom=rp.blend(bottom,black,rp.random_float())\n",
    "        top=rp.blend(top,black,rp.random_float())\n",
    "    return (a**exp * b**exp *c**exp * d**exp * brightness).clamp(0,99).tanh()\n",
    "\n",
    "learnable_image_a=lambda: image_a()\n",
    "learnable_image_b=lambda: image_b()\n",
    "learnable_image_c=lambda: image_c()\n",
    "learnable_image_d=lambda: image_d()\n",
    "learnable_image_z=lambda: simulate_overlay(image_a(), image_b(), image_c(), image_d())\n",
    "\n",
    "params=chain(\n",
    "    image_a.parameters(),\n",
    "    image_b.parameters(),\n",
    "    image_c.parameters(),\n",
    "    image_d.parameters(),\n",
    ")\n",
    "optim=torch.optim.SGD(params,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKCQQg9teoMt"
   },
   "outputs": [],
   "source": [
    "labels=[label_a, label_b, label_c, label_d, label_z]\n",
    "learnable_images=[learnable_image_a,learnable_image_b,learnable_image_c,learnable_image_d,learnable_image_z]\n",
    "\n",
    "#The weight coefficients for each prompt. For example, if we have [1,1,1,1,5], then the hidden prompt (prompt_z) will be prioritized\n",
    "weights=[1,1,1,1,3]\n",
    "\n",
    "weights=rp.as_numpy_array(weights)\n",
    "weights=weights/weights.sum()\n",
    "weights=weights*len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWpLV_Toertv"
   },
   "outputs": [],
   "source": [
    "#For saving a timelapse\n",
    "ims=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6-FlVZPes09"
   },
   "outputs": [],
   "source": [
    "def get_display_image():\n",
    "    return rp.tiled_images(\n",
    "        [\n",
    "            *[rp.as_numpy_image(image()) for image in learnable_images[:-1]],\n",
    "            rp.as_numpy_image(learnable_image_z()),\n",
    "        ],\n",
    "        length=len(learnable_images),\n",
    "        border_thickness=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "bB-Uv4Y5et8J",
    "outputId": "0cf6d601-40a6-4140-f0c5-e4720d389822"
   },
   "outputs": [],
   "source": [
    "NUM_ITER=10000\n",
    "\n",
    "#Set the minimum and maximum noise timesteps for the dream loss (aka score distillation loss)\n",
    "s.max_step=MAX_STEP=990\n",
    "s.min_step=MIN_STEP=10 \n",
    "\n",
    "display_eta=rp.eta(NUM_ITER, title='Status: ')\n",
    "\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "print('Every %i iterations we display an image in the form [image_a, image_b, image_c, image_d, image_z] where'%DISPLAY_INTERVAL)\n",
    "print('    image_z = image_a * image_b * image_c * image_d')\n",
    "print()\n",
    "print('Interrupt the kernel at any time to return the currently displayed image')\n",
    "print('You can run this cell again to resume training later on')\n",
    "print()\n",
    "print('Please expect this to take hours to get good images (especially on the slower Colab GPU\\'s! The longer you wait the better they\\'ll be')\n",
    "\n",
    "try:\n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num) #Print the remaining time\n",
    "\n",
    "        preds=[]\n",
    "        for label,learnable_image,weight in rp.random_batch(list(zip(labels,learnable_images,weights)), batch_size=1):\n",
    "            pred=s.train_step(\n",
    "                label.embedding,\n",
    "                learnable_image()[None],\n",
    "\n",
    "                #PRESETS (uncomment one):\n",
    "                noise_coef=.1*weight,guidance_scale=60,#10\n",
    "                # noise_coef=0,image_coef=-.01,guidance_scale=50,\n",
    "                # noise_coef=0,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.010,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1*weight, image_coef=-.005*weight, guidance_scale=50,\n",
    "            )\n",
    "            preds+=list(pred)\n",
    "\n",
    "        im = get_display_image()\n",
    "        ims.append(im)\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num%(DISPLAY_INTERVAL*50):\n",
    "                #Wipe the slate every 50 displays so they don't get cut off\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "\n",
    "            if not iter_num%DISPLAY_INTERVAL:\n",
    "                rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Interrupted early at iteration %i'%iter_num)\n",
    "    im = get_display_image()\n",
    "    ims.append(im)\n",
    "    rp.display_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwhrgJiie3mX"
   },
   "outputs": [],
   "source": [
    "print('Image A')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_a()))\n",
    "\n",
    "print('Image B')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_b()))\n",
    "\n",
    "print('Image C')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_c()))\n",
    "\n",
    "print('Image D')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_d()))\n",
    "\n",
    "print('Image Z')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_z()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YJJw4dXe4JJ"
   },
   "outputs": [],
   "source": [
    "def save_run(name):\n",
    "    folder=\"untracked/hidden_character_runs/%s\"%name\n",
    "    if rp.path_exists(folder):\n",
    "        folder+='_%i'%time.time()\n",
    "    rp.make_directory(folder)\n",
    "    ims_names=['ims_%04i.png'%i for i in range(len(ims))]\n",
    "    print()\n",
    "    rp.save_video_mp4(ims,folder+'.mp4',video_bitrate='high')\n",
    "    with rp.SetCurrentDirectoryTemporarily(folder):\n",
    "        rp.save_images(ims,ims_names,show_progress=True)\n",
    "        pass\n",
    "    print('Saved timelapse to folder:',repr(folder))\n",
    "    \n",
    "save_run(title) #You can give it a good custom name if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "GUIDANCE=32 ;  TIMESTEPS=range(100, 75, -1) ;  EMA_ALPHA=.2 ;  ORIG_ALPHA=.05 ; NEG='blurry unfocused low quality bokeh, depth of field' ;#Medium Light\n",
    "GUIDANCE=32 ;  TIMESTEPS=range(100, 75, -1) ;  EMA_ALPHA=.2 ;  ORIG_ALPHA=.015 ; NEG='blurry unfocused low quality bokeh, depth of field' ;#Medium Light\n",
    "GUIDANCE=32 ;  TIMESTEPS=range(300, 75, -5) ;  EMA_ALPHA=.2 ;  ORIG_ALPHA=.055 ; NEG='blurry unfocused low quality bokeh, depth of field' ;#Medium Harsh\n",
    "# GUIDANCE=32 ;  TIMESTEPS=range(300, 75, -5) ;  EMA_ALPHA=.2 ;  ORIG_ALPHA=.025 ; NEG='blurry unfocused low quality bokeh, depth of field' ;#Medium Harsh\n",
    "# GUIDANCE=16 ;  TIMESTEPS=range(300, 200, -1) ;  EMA_ALPHA=.2 ;  ORIG_ALPHA=.025 ; NEG='blurry unfocused low quality bokeh, depth of field' ;#Medium Harsh\n",
    "# GUIDANCE=32 ;  TIMESTEPS=range(500, 10, -10) ;  EMA_ALPHA=.2 ;  ORIG_ALPHA=.01 ; NEG='blurry unfocused low quality bokeh, depth of field' ; #Aggressive\n",
    "# GUIDANCE=4 ;  TIMESTEPS=range(999, 500, -10) ;  EMA_ALPHA=1 ;  ORIG_ALPHA=0 ; NEG='oversaturated, blurry unfocused low quality bokeh, depth of field, unrealistic, abstract, deep fried' ; #Complete\n",
    "GUIDANCE=32 ;  TIMESTEPS=range(999, 10, -2) ;  EMA_ALPHA=.05 ;  ORIG_ALPHA=0 ; NEG='' ; #Complete\n",
    "# GUIDANCE=7 ;  TIMESTEPS=range(999, 10, -2) ;  EMA_ALPHA=.1 ;  ORIG_ALPHA=0 ; NEG='' ; #Complete\n",
    "\n",
    "COMPLETE=True#If this is set to True, generate the images totally from scratch using the default method\n",
    "COMPLETE=False\n",
    "\n",
    "#Show a timelapse of each diffusion process. Can take a while to load into the notebook.\n",
    "SHOW_ANIMS=True\n",
    "# SHOW_ANIMS=False \n",
    "\n",
    "@rp.monkey_patch(sd.StableDiffusion)\n",
    "def redenoise_latent(self,                   text_embeddings:torch.Tensor,\n",
    "                   latent:torch.Tensor,\n",
    "                   guidance_scale:float=GUIDANCE,\n",
    "                   t:int=None,):\n",
    "        \n",
    "        if t is None:\n",
    "            t = torch.randint(self.min_step, self.max_step + 1, [1], dtype=torch.long, device=self.device)\n",
    "\n",
    "        assert 0<=t<self.num_train_timesteps, 'invalid timestep t=%i'%t\n",
    "\n",
    "        latents=latent[None]\n",
    "\n",
    "        \n",
    "        # predict the noise residual with unet, NO grad!\n",
    "        with torch.no_grad():\n",
    "            # add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            #This is the only place we use the scheduler...the add_noise function. What's more...it's totally generic! The scheduler doesn't impact the implementation of train_step...\n",
    "            if t==999:\n",
    "                latents_noisy=noise+0 #Eh sometimes I want to have complete noise\n",
    "            else:\n",
    "                latents_noisy = self.add_noise(latents, noise, t) #The add_noise function is identical for PNDM, DDIM, and DDPM schedulers in the diffusers library\n",
    "            #TODO: Expand this add_noise function, and put it in this class. That way we don't need the scheduler...and we can also add an inverse function, which is what I need for previews...that subtracts noise...\n",
    "            #Also, create a dream-loss-based image gen example notebook...\n",
    "\n",
    "            # pred noise\n",
    "            latent_model_input = torch.cat([latents_noisy] * 2)\n",
    "            noise_pred = self.predict_noise(latent_model_input, text_embeddings, t)\n",
    "\n",
    "                        \n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "            latent_pred = self.remove_noise(latents_noisy, noise_pred, t)\n",
    "            # rp.ic(latent_pred.shape)\n",
    "            output = latent_pred[0]\n",
    "            \n",
    "            # latent_pred = self.decode_latents(latent_pred)[0]\n",
    "\n",
    "        return latent_pred[0]\n",
    "\n",
    "\n",
    "    \n",
    "def denoise_l(latent,label,T):\n",
    "    return s.redenoise_latent(latent=latent,\n",
    "                              text_embeddings=label.embedding,\n",
    "                              t=torch.tensor(T, dtype=torch.int)\n",
    "                             )\n",
    "    \n",
    "def get_ii_seqo(w=learnable_image_z, lw=label_z):\n",
    "    seqo=[]\n",
    "    \n",
    "    if COMPLETE:\n",
    "        out=lw.get_sample_image()\n",
    "        rp.display_image(out)\n",
    "        return out,[out]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # w,lw=learnable_image_x,label_x\n",
    "        # w,lw=learnable_image_y,label_y\n",
    "        # w,lw=learnable_image_z,label_z\n",
    "\n",
    "        lw=NegativeLabel(lw.name,NEG)\n",
    "\n",
    "        w=w()\n",
    "        i=w\n",
    "        i = F.interpolate(i[None], (512, 512), mode='bilinear', align_corners=False)[0]\n",
    "        l=s.encode_img(i)\n",
    "        ol=l\n",
    "\n",
    "        rp.display_image(rp.as_numpy_image(i))\n",
    "\n",
    "        # for T in [10]*100:\n",
    "        # for T in [100,100,100,100,100]*10:\n",
    "        # for T in [100,100,100,100,100]:\n",
    "        rp.tic()\n",
    "        # for T in list(range(999, 0, -10)):\n",
    "        # for T in list(range(500, 0, -10)):\n",
    "        # for T in list(range(999, 0, -50)):\n",
    "        # for T in list(range(100, 0, -1)):\n",
    "        did_999=False\n",
    "        for T in list(TIMESTEPS):\n",
    "            if T==999:\n",
    "                if did_999: continue\n",
    "                did_999=True\n",
    "                \n",
    "            if T!=999:\n",
    "                T=rp.random_element(set(TIMESTEPS)-{999})\n",
    "            # torch.manual_seed(298)\n",
    "\n",
    "            dl=denoise_l(l, lw, T)\n",
    "            # l=rp.blend(l,dl,1)#Stick to the previous, EMA, make it smooth! Less variance\n",
    "            # l=rp.blend(l,dl,.2)#Stick to the previous, EMA, make it smooth! Less variance\n",
    "            # l=rp.blend(l,dl,.01)#Stick to the previous, EMA, make it smooth! Less variance\n",
    "            if T!=999:\n",
    "                l=rp.blend(l,dl,EMA_ALPHA)#Stick to the previous, EMA, make it smooth! Less variance\n",
    "\n",
    "                # l=rp.blend(l,ol,.05)#Stick to the original\n",
    "                l=rp.blend(l,ol,ORIG_ALPHA)#Stick to the original\n",
    "            else:\n",
    "                l=dl\n",
    "                # l=dl\n",
    "\n",
    "            from IPython.display import clear_output\n",
    "            # clear_output()\n",
    "\n",
    "            ii=rp.as_numpy_image(s.decode_latent(l))\n",
    "            print(T)\n",
    "            seqo.append(rp.cv_resize_image(ii,.5))\n",
    "            if rp.toc()>10:\n",
    "                rp.display_image(ii)\n",
    "                rp.tic()\n",
    "        rp.display_image(ii)\n",
    "        return ii,seqo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI,seqo=get_ii_seqo(learnable_image_a,label_a)\n",
    "if SHOW_ANIMS:rp.display_image_slideshow(seqo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI,seqo=get_ii_seqo(learnable_image_b,label_b)\n",
    "if SHOW_ANIMS:rp.display_image_slideshow(seqo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI,seqo=get_ii_seqo(learnable_image_c,label_c)\n",
    "if SHOW_ANIMS:rp.display_image_slideshow(seqo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI,seqo=get_ii_seqo(learnable_image_d,label_d)\n",
    "if SHOW_ANIMS:rp.display_image_slideshow(seqo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZI,seqo=get_ii_seqo(learnable_image_z,label_z)\n",
    "if SHOW_ANIMS:rp.display_image_slideshow(seqo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "\n",
    "rp.display_image(rp.tiled_images([AI,BI,CI,DI,ZI]))\n",
    "\n",
    "\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "\n",
    "down_images=rp.as_torch_images(rp.as_numpy_array([rp.cv_resize_image(rp.as_numpy_image(x),(SIZE,SIZE)) for x in [AI,BI,CI,DI,ZI]])).to(s.device)\n",
    "\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "\n",
    "shlump=[]\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "\n",
    "from source.msssim import msssim\n",
    "\n",
    "# WEIGHTS=[1,2,1.5,2] #If one of the images looks sus, add more weight!\n",
    "# WEIGHTS=[1,1,1,1,5] #If one of the images looks sus, add more weight!\n",
    "WEIGHTS=[1,1,1,1,2] #If one of the images looks sus, add more weight!\n",
    "# WEIGHTS=[0,0,0,0,1] #Fix Z and only go to it\n",
    "# WEIGHTS=[1,1,1,.3,1] #Fix Z and only go to it\n",
    "# WEIGHTS=[1,1,1,.3,5] #Fix Z and only go to it\n",
    "# WEIGHTS=[1,1,1,1,10] #Fix Z and only go to it\n",
    "WEIGHTS=rp.as_numpy_array(WEIGHTS)\n",
    "WEIGHTS=WEIGHTS/WEIGHTS.sum()\n",
    "\n",
    "MSSSIM_COEF = 0 ; MSE_COEF = 1\n",
    "MSSSIM_COEF = .2 ; MSE_COEF = 1\n",
    "# MSSSIM_COEF = .5 ; MSE_COEF = 1\n",
    "# MSSSIM_COEF = 1 ; MSE_COEF = 1\n",
    "\n",
    "for _ in range(10000):\n",
    "    if not _%500:\n",
    "        with torch.no_grad():\n",
    "            shlumper=rp.tiled_images(\n",
    "                [\n",
    "                    rp.as_numpy_image(learnable_image_a()),\n",
    "                    rp.as_numpy_image(learnable_image_b()),\n",
    "                    rp.as_numpy_image(learnable_image_c()),\n",
    "                    rp.as_numpy_image(learnable_image_d()),\n",
    "                    rp.as_numpy_image(learnable_image_z()),\n",
    "                ],\n",
    "                border_thickness=0,\n",
    "                length=4\n",
    "            )\n",
    "        rp.display_image(shlumper)\n",
    "        \n",
    "    DO_ALL=False #Not enough vram\n",
    "\n",
    "    limage=[\n",
    "        learnable_image_a,\n",
    "        learnable_image_b,\n",
    "        learnable_image_c,\n",
    "        learnable_image_d,\n",
    "        learnable_image_z,\n",
    "    ]\n",
    "    index=_%len(limage)\n",
    "    \n",
    "    if not WEIGHTS[index]:continue\n",
    "    \n",
    "    limage=limage[index]\n",
    "    limage=limage()\n",
    "    dimage=down_images[index]\n",
    "    loss=0\n",
    "\n",
    "\n",
    "    if MSE_COEF   : loss=loss+  ((limage-dimage)**2).mean()       * MSE_COEF\n",
    "    if MSSSIM_COEF: loss=loss-  msssim(limage[None],dimage[None]) * MSSSIM_COEF\n",
    "\n",
    "    total_loss=loss*4*WEIGHTS[index]\n",
    "        \n",
    "    total_loss=total_loss*10000\n",
    "        \n",
    "    total_loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    if not _%51:\n",
    "        print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13511603d504480489e9de201dd85211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_875b8337c4304ae0b9ea63eb7435d3ba",
      "max": 23,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f767f331f3af45d4a3814a5447d29004",
      "value": 23
     }
    },
    "19d10e0e23fa460c8ba196c1691ecef3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38124b85cb824de2a2640dfede710372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "875b8337c4304ae0b9ea63eb7435d3ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a95ebc987364a3cbbc27866ff8c964b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4b3425062c041b3aa5db68a73ea0a01",
      "placeholder": "​",
      "style": "IPY_MODEL_eb7e46a3bff547ba8d07ebc87eddd9a0",
      "value": "Fetching 23 files: 100%"
     }
    },
    "df5753e3d2b54078a5ff58f1f0f5de78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb7e46a3bff547ba8d07ebc87eddd9a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec572d79f75648e69c18f8fbb8445abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a95ebc987364a3cbbc27866ff8c964b",
       "IPY_MODEL_13511603d504480489e9de201dd85211",
       "IPY_MODEL_f72921cdc424455e8d2e86b0ea07a29b"
      ],
      "layout": "IPY_MODEL_df5753e3d2b54078a5ff58f1f0f5de78"
     }
    },
    "f4b3425062c041b3aa5db68a73ea0a01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72921cdc424455e8d2e86b0ea07a29b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19d10e0e23fa460c8ba196c1691ecef3",
      "placeholder": "​",
      "style": "IPY_MODEL_38124b85cb824de2a2640dfede710372",
      "value": " 23/23 [00:00&lt;00:00, 1628.58it/s]"
     }
    },
    "f767f331f3af45d4a3814a5447d29004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
