{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujL4Q8IHPhm0"
   },
   "outputs": [],
   "source": [
    "import rp\n",
    "import source.peekaboo as peekaboo\n",
    "from source.peekaboo import run_peekaboo\n",
    "from source.clip import get_clip_logits\n",
    "from source.stable_diffusion import StableDiffusion\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=rp.select_torch_device()\n",
    "StableDiffusion(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Sgqq2J32Qzs",
    "outputId": "eee9073d-e09f-49ca-ca1a-5bb8b8d5e19c"
   },
   "outputs": [],
   "source": [
    "#Mario vs Luigi Part 1\n",
    "results_collection=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_logits_per_image(prompt, images):\n",
    "    return [get_clip_logits(image, [prompt])[0] for image in images]\n",
    "\n",
    "def random_colors(length=100):\n",
    "    return [rp.random_rgb_float_color() for _ in range(length)]\n",
    "\n",
    "def get_score(foreground, alpha, prompt:str, colors:list):\n",
    "    \n",
    "    alpha=alpha>.5\n",
    "    rp.display_image(rp.blend_images(rp.random_rgb_float_color(),rp.cv_resize_image(foreground,rp.get_image_dimensions(alpha)),alpha))\n",
    "    alpha=rp.cv_dilate(alpha,5,circular=True)\n",
    "    \n",
    "    assert rp.is_image(foreground) and rp.is_image(alpha)\n",
    "    images = [rp.blend_images(foreground, color, alpha) for color in colors]\n",
    "    scores = get_clip_logits_per_image(prompt, images)\n",
    "    score = rp.mean(scores)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_results(results):\n",
    "    colors = random_colors()\n",
    "    def score(result):\n",
    "        output = get_score(foreground = result.image,\n",
    "                         alpha = result.alphas[0],\n",
    "                         prompt = result.p_name,\n",
    "                         colors = colors,\n",
    "                        )\n",
    "        print(output)\n",
    "        return output\n",
    "    scores = list(map(score,results))\n",
    "    scores, results = rp.sync_sort(scores, results)\n",
    "    \n",
    "    #First is best\n",
    "    scores, results = scores[::-1], results[::-1]\n",
    "    \n",
    "    return scores, results\n",
    "\n",
    "def display_ranked_results(scores, results):\n",
    "    for score, result in zip(scores, results):\n",
    "        rp.display_image(\n",
    "            rp.labeled_image(\n",
    "                rp.horizontally_concatenated_images(\n",
    "                    rp.cv_resize_image(result.image, (256, 256)), result.alphas[0]\n",
    "                ),\n",
    "                \"%s : %f\" % (result.p_name, score),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnails=[]\n",
    "def get_result_thumbnail(results):\n",
    "    alpha, image = results.alphas[0], results.image\n",
    "    height = width = 512 \n",
    "    image = rp.cv_resize_image(image, (height, width))\n",
    "    alpha = rp.cv_resize_image(alpha, (height, width),interp='nearest')\n",
    "    method = \" + \".join(\n",
    "        [\n",
    "            *([\"CLIP\"] if results.clip_coef else []),\n",
    "            *([\"StableDreamLoss\"] if results.use_stable_dream_loss else []),\n",
    "        ]\n",
    "    )\n",
    "    path = rp.get_relative_path(results.output_folder)\n",
    "    name = results.p_name\n",
    "    settings=[\n",
    "        'representation',\n",
    "        'LEARNING_RATE',\n",
    "        'NUM_ITER',\n",
    "        'GRAVITY',\n",
    "        'clip_coef',\n",
    "        'use_stable_dream_loss',\n",
    "        'GUIDANCE_SCALE',\n",
    "        'min_step',\n",
    "        'max_step',\n",
    "        'clip_coef',\n",
    "    ]\n",
    "    settings=[x+': '+str(results[x]) for x in settings]\n",
    "    \n",
    "    \n",
    "    text=rp.line_join([name, ' ', method, ' ', path, '', 'Settings:',*settings])\n",
    "    text=rp.wrap_string_to_width(text,60)\n",
    "    text = rp.cv_text_to_image(text,monospace=False)\n",
    "    text = rp.resize_image_to_fit(text, height, width)\n",
    "    image=rp.horizontally_concatenated_images(image,alpha,text)\n",
    "    \n",
    "    out_dir='thumbnails_data6'\n",
    "    rp.make_directory(out_dir)\n",
    "    out_name='%s ____ %s ____ %i.png'%(method, name,len(rp.get_all_files(out_dir)))\n",
    "    out_path=rp.path_join(out_dir,out_name)\n",
    "    rp.save_image(image,out_path)\n",
    "    \n",
    "    thumbnails.append(image)\n",
    "    rp.display_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_input_data=[       [ [\n",
    "            # \" Harry Potter boy - daniel radcliffe - man with glasses\",\n",
    "            \"Hermoine Granger - Emma Watson - Girl with long hair\",\n",
    "            # \"Hermoine\",\n",
    "            # \"Harry Potter\",\n",
    "            # \"Daniel Radcliffe\",\n",
    "            # \"Emma Watson\",\n",
    "        ],\n",
    "        [\n",
    "            \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "            # \"https://i.guim.co.uk/img/media/380500f7fd9321a6cfd1bd1a3b3f104ad9797bb7/0_0_2000_1199/master/2000.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=f0d0ecb5a0c33aa91be270b08619e8d7\",\n",
    "        ],]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import icecream\n",
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from easydict import EasyDict\n",
    "from IPython.display import clear_output\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "import source.stable_diffusion as sd\n",
    "from source.bilateral_blur import BilateralProxyBlur\n",
    "from source.learnable_textures import (LearnableImageFourier,\n",
    "                                       LearnableImageFourierBilateral,\n",
    "                                       LearnableImageRaster,\n",
    "                                       LearnableImageRasterBilateral,\n",
    "                                       LearnableTexturePackFourier,\n",
    "                                       LearnableTexturePackRaster,\n",
    "                                      LearnableImageRasterSigmoided)\n",
    "\n",
    "from typing import Union, List, Optional\n",
    "import rp\n",
    "from easydict import EasyDict\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "\n",
    "from source.stable_diffusion_labels import get_mean_embedding, BaseLabel, SimpleLabel, MeanLabel\n",
    "\n",
    "def make_learnable_image(height, width, num_channels, foreground=None, bilateral_kwargs:dict={}, representation = 'fourier'):\n",
    "    #Here we determine our image parametrization schema\n",
    "    bilateral_blur =  BilateralProxyBlur(foreground,**bilateral_kwargs)\n",
    "    if representation=='fourier bilateral':\n",
    "        return LearnableImageFourierBilateral(bilateral_blur,num_channels) #A neural neural image + bilateral filter\n",
    "    elif representation=='raster bilateral':\n",
    "        return LearnableImageRasterBilateral(bilateral_blur,num_channels) #A regular image + bilateral filter\n",
    "    elif representation=='fourier':\n",
    "        return LearnableImageFourier(height,width,num_channels) #A neural neural image\n",
    "    elif representation=='raster':\n",
    "        return LearnableImageRasterSigmoided(height,width,num_channels) #A regular image\n",
    "    else:\n",
    "        assert False, 'Invalid method: '+representation\n",
    "\n",
    "def blend_torch_images(foreground, background, alpha):\n",
    "    #Input assertions\n",
    "    assert foreground.shape==background.shape\n",
    "    C,H,W=foreground.shape\n",
    "    assert alpha.shape==(H,W), 'alpha is a matrix'\n",
    "    \n",
    "    return foreground*alpha + background*(1-alpha)\n",
    "\n",
    "class PeekabooSegmenter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image:np.ndarray,\n",
    "                 labels:List['BaseLabel'],\n",
    "                 size:int=256,\n",
    "                 name:str='Untitled',\n",
    "                 bilateral_kwargs:dict={},\n",
    "                 representation = 'fourier bilateral',\n",
    "                 min_step=None,\n",
    "                 max_step=None,\n",
    "                ):\n",
    "        \n",
    "        s=sd._get_stable_diffusion_singleton()\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        height=width=size #We use square images for now\n",
    "        \n",
    "        assert all(issubclass(type(label),BaseLabel) for label in labels)\n",
    "        assert len(labels), 'Must have at least one class to segment'\n",
    "        \n",
    "        self.height=height\n",
    "        self.width=width\n",
    "        self.labels=labels\n",
    "        self.name=name\n",
    "        self.representation=representation\n",
    "        self.min_step=s.min_step if min_step is None else min_step\n",
    "        self.max_step=s.max_step if max_step is None else max_step\n",
    "        \n",
    "        assert rp.is_image(image), 'Input should be a numpy image'\n",
    "        image=rp.cv_resize_image(image,(height,width))\n",
    "        image=rp.as_rgb_image(image) #Make sure it has 3 channels in HWC form\n",
    "        image=rp.as_float_image(image) #Make sure it's values are between 0 and 1\n",
    "        assert image.shape==(height,width,3) and image.min()>=0 and image.max()<=1\n",
    "        self.image=image\n",
    "        \n",
    "        self.foreground=rp.as_torch_image(image).to(s.device) #Convert the image to a torch tensor in CHW form\n",
    "        assert self.foreground.shape==(3, height, width)\n",
    "        \n",
    "        self.background=self.foreground*0 #The background will be a solid color for now\n",
    "        \n",
    "        self.alphas=make_learnable_image(height,width,num_channels=self.num_labels,foreground=self.foreground,representation=self.representation,bilateral_kwargs=bilateral_kwargs)\n",
    "            \n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.labels)\n",
    "            \n",
    "    def set_background_color(self, color):\n",
    "        r,g,b = color\n",
    "        assert 0<=r<=1 and 0<=g<=1 and 0<=b<=1\n",
    "        self.background[0]=r\n",
    "        self.background[1]=g\n",
    "        self.background[2]=b\n",
    "        \n",
    "    def randomize_background(self):\n",
    "        self.set_background_color(rp.random_rgb_float_color())\n",
    "        \n",
    "    def forward(self, alphas=None, return_alphas=False):        \n",
    "        s=sd._get_stable_diffusion_singleton()\n",
    "        \n",
    "        try:\n",
    "            old_min_step=s.min_step\n",
    "            old_max_step=s.max_step\n",
    "            s.min_step=self.min_step\n",
    "            s.max_step=self.max_step\n",
    "\n",
    "            output_images = []\n",
    "\n",
    "            if alphas is None:\n",
    "                alphas=self.alphas()\n",
    "\n",
    "            assert alphas.shape==(self.num_labels, self.height, self.width)\n",
    "            assert alphas.min()>=0 and alphas.max()<=1\n",
    "\n",
    "            for alpha in alphas:\n",
    "                output_image=blend_torch_images(foreground=self.foreground, background=self.background, alpha=alpha)\n",
    "                output_images.append(output_image)\n",
    "\n",
    "            output_images=torch.stack(output_images)\n",
    "\n",
    "            assert output_images.shape==(self.num_labels, 3, self.height, self.width) #In BCHW form\n",
    "\n",
    "            if return_alphas:\n",
    "                return output_images, alphas\n",
    "            else:\n",
    "                return output_images\n",
    "\n",
    "        finally:\n",
    "            old_min_step=s.min_step\n",
    "            old_max_step=s.max_step \n",
    "\n",
    "def display(self):\n",
    "    #This is a method of PeekabooSegmenter, but can be changed without rewriting the class if you want to change the display\n",
    "\n",
    "    colors = [(1,1,1), (0,0,0), (1,0,0),(0,1,0),(0,0,1)]#(1,0,0), (0,1,0), (0,0,1)] #Colors used to make the display\n",
    "    # colors = [rp.random_rgb_float_color() for _ in range(3)]\n",
    "    with torch.no_grad():\n",
    "        alphas = rp.as_numpy_array(self.alphas())\n",
    "    image = self.image\n",
    "    assert alphas.shape==(self.num_labels, self.height, self.width)\n",
    "\n",
    "    composites = []\n",
    "    for color in colors:\n",
    "        self.set_background_color(color)\n",
    "        column=rp.as_numpy_images(self(self.alphas()))\n",
    "        composites.append(column)\n",
    "\n",
    "    label_names=[label.name for label in self.labels]\n",
    "\n",
    "    stats_lines = [\n",
    "        self.name,\n",
    "        '',\n",
    "        'H,W = %ix%i'%(self.height,self.width),\n",
    "    ]\n",
    "\n",
    "    def try_add_stat(stat_format, var_name):\n",
    "        if var_name in globals():\n",
    "            stats_line=stat_format%globals()[var_name]\n",
    "            stats_lines.append(stats_line)\n",
    "\n",
    "    try_add_stat('Gravity: %.2e','GRAVITY'   )\n",
    "    try_add_stat('Batch Size: %i','BATCH_SIZE')\n",
    "    try_add_stat('Iter: %i','iter_num')\n",
    "    try_add_stat('Image Name: %s','image_filename')\n",
    "    try_add_stat('Learning Rate: %.2e','LEARNING_RATE')\n",
    "    try_add_stat('Guidance: %i%%','GUIDANCE_SCALE')\n",
    "\n",
    "    stats_image=rp.labeled_image(self.image, rp.line_join(stats_lines), \n",
    "                                 size=15*len(stats_lines), \n",
    "                                 position='bottom', align='center')\n",
    "\n",
    "    composite_grid=rp.grid_concatenated_images([\n",
    "        rp.labeled_images(alphas,label_names),\n",
    "        *composites\n",
    "    ])\n",
    "    \n",
    "    assert rp.is_image(self.image)\n",
    "    assert rp.is_image(alphas[0])\n",
    "    assert rp.is_image(composites[0][0])\n",
    "    assert rp.is_image(composites[1][0])\n",
    "    assert rp.is_image(composites[2][0])\n",
    "\n",
    "    output_image = rp.labeled_image(\n",
    "        rp.tiled_images(\n",
    "            rp.labeled_images(\n",
    "                [\n",
    "                    self.image,\n",
    "                    alphas[0],\n",
    "\n",
    "                    composites[0][0],\n",
    "                    composites[1][0],\n",
    "                    composites[2][0],\n",
    "                    composites[3][0],\n",
    "                    composites[4][0],\n",
    "                ],\n",
    "                [\n",
    "                    \"Input Image\",\n",
    "                    \"Alpha Map\",\n",
    "                    \"Background #1\",\n",
    "                    \"Background #2\",\n",
    "                    \"Background #3\",\n",
    "                    \"Background #4\",\n",
    "                    \"Background #5\",\n",
    "                ],\n",
    "            ),\n",
    "            length=2 + len(composites),\n",
    "            border_thickness=0,\n",
    "        ),\n",
    "        label_names[0],\n",
    "    )\n",
    "\n",
    "\n",
    "    # output_image = rp.horizontally_concatenated_images(stats_image, composite_grid)\n",
    "\n",
    "    rp.display_image(output_image)\n",
    "\n",
    "    return output_image\n",
    "\n",
    "PeekabooSegmenter.display=display\n",
    "    \n",
    "def log_cell(cell_title):\n",
    "    rp.fansi_print(\"<Cell: %s>\"%cell_title, 'cyan', 'underlined')\n",
    "    # rp.ptoc()\n",
    "def log(x):\n",
    "    x=str(x)\n",
    "    rp.fansi_print(x, 'yellow')\n",
    "\n",
    "class PeekabooResults(EasyDict):\n",
    "    #Acts like a dict, except you can read/write parameters by doing self.thing instead of self['thing']\n",
    "    pass\n",
    "\n",
    "def save_peekaboo_results(results,new_folder_path):\n",
    "    assert not rp.folder_exists(new_folder_path), 'Please use a different name, not %s'%new_folder_path\n",
    "    rp.make_folder(new_folder_path)\n",
    "    with rp.SetCurrentDirectoryTemporarily(new_folder_path):\n",
    "        log(\"Saving PeekabooResults to \"+new_folder_path)\n",
    "        params={}\n",
    "        rp.save_json(params,'params.json',pretty=True)\n",
    "        for key in results:\n",
    "            value=results[key]\n",
    "            if rp.is_image(value): \n",
    "                #Save a single image\n",
    "                rp.save_image(value,key+'.png')\n",
    "            elif isinstance(value, np.ndarray) and rp.is_image(value[0]):\n",
    "                #Save a folder of images\n",
    "                rp.make_directory(key)\n",
    "                with rp.SetCurrentDirectoryTemporarily(key):\n",
    "                    \n",
    "                    \n",
    "                    names=[]\n",
    "                    for i in range(len(value)):\n",
    "                        name='%03i.png'%i\n",
    "                        names.append(name)\n",
    "                        # rp.save_image(value[i],str(i)+'.png')\n",
    "                    rp.save_images(value,names,show_progress=True)\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                #Save a generic numpy array\n",
    "                np.save(key+'.npy',value) \n",
    "            else:\n",
    "\n",
    "                import json\n",
    "                try:\n",
    "                    json.dumps({key:value})\n",
    "                    #Assume value is json-parseable\n",
    "                    params[key]=value\n",
    "                except Exception:\n",
    "                    params[key]=str(value)\n",
    "        log(\"Done saving PeekabooResults to \"+new_folder_path+\"!\")\n",
    "        \n",
    "        \n",
    "class PeekabooResult:\n",
    "    def __init__(self,root):\n",
    "        self.root=root\n",
    "\n",
    "    def sub(self,path):\n",
    "        return rp.path_join(self.root,path)\n",
    "\n",
    "    def img(self,path):\n",
    "        return rp.as_float_image(rp.load_image(self.sub(path),use_cache=True))\n",
    "\n",
    "    @property\n",
    "    def is_valid(self):\n",
    "        try:\n",
    "            #Are all the associated paths there?\n",
    "            self.params\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    @rp.memoized_property\n",
    "    def image_path(self):\n",
    "        return self.params.image_path\n",
    "\n",
    "    @rp.memoized_property\n",
    "    def image_name(self):\n",
    "        return rp.get_file_name(self.image_path)\n",
    "\n",
    "    @rp.memoized_property\n",
    "    def name(self):\n",
    "        return self.params.p_name\n",
    "\n",
    "    @rp.memoized_property\n",
    "    def params(self):\n",
    "        return EasyDict(rp.load_json(self.sub('params.json')))\n",
    "\n",
    "    @rp.memoized_property\n",
    "    def image(self):\n",
    "        return rp.as_rgb_image(rp.as_float_image(self.img('image.png')))\n",
    "    \n",
    "    @rp.memoized_property\n",
    "    def scaled_image(self):\n",
    "        return rp.cv_resize_image(self.image,rp.get_image_file_dimensions(self.alpha_path))\n",
    "\n",
    "    @rp.memoized_property\n",
    "    def alpha(self):\n",
    "        return rp.as_grayscale_image(rp.as_float_image(self.img('alphas/0.png')))\n",
    "    \n",
    "    @rp.memoized_property\n",
    "    def alpha_path(self):\n",
    "        return self.sub('alphas/0.png')\n",
    "\n",
    "    @rp.memoized_property\n",
    "    def preview_image(self):\n",
    "        return self.img('preview_image.png')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'PeekabooResult(%s)'%(self.name)\n",
    "        \n",
    "def make_image_square(image:np.ndarray, method='crop')->np.ndarray:\n",
    "    #Takes any image and makes it into a 512x512 square image with shape (512,512,3)\n",
    "    assert rp.is_image(image)\n",
    "    assert method in ['crop','scale']\n",
    "    image=rp.as_rgb_image(image)\n",
    "    \n",
    "    height, width = rp.get_image_dimensions(image)\n",
    "    min_dim=min(height,width)\n",
    "    max_dim=max(height,width)\n",
    "    \n",
    "    if method=='crop':\n",
    "        return make_image_square(rp.crop_image(image, min_dim, min_dim, origin='center'),'scale')\n",
    "    if method=='scale':\n",
    "        return rp.resize_image(image, (512,512))\n",
    "                    \n",
    "\n",
    "def run_peekaboo(name:str, image:Union[str,np.ndarray], label:Optional['BaseLabel']=None,\n",
    "                \n",
    "                #Peekaboo Hyperparameters:\n",
    "                GRAVITY=1e-1/2, # This is the one that needs the most tuning, depending on the prompt...\n",
    "                #   ...usually one of the following GRAVITY will work well: 1e-2, 1e-1/2, 1e-1, or 1.5*1e-1\n",
    "                NUM_ITER=300,       # 300 is usually enough\n",
    "                LEARNING_RATE=1e-5, # Can be larger if not using neural neural textures (aka when representation is raster)\n",
    "                BATCH_SIZE=1,       # Doesn't make much difference, larger takes more vram\n",
    "                GUIDANCE_SCALE=100, # The defauly value from the DreamFusion paper\n",
    "                bilateral_kwargs=dict(kernel_size = 3,\n",
    "                                      tolerance = .08,\n",
    "                                      sigma = 5,\n",
    "                                      iterations=40,\n",
    "                                     ),\n",
    "                square_image_method='crop', #Can be either 'crop' or 'scale' - how will we square the input image?\n",
    "                representation='fourier bilateral', #Can be 'fourier bilateral', 'raster bilateral', 'fourier', or 'raster'\n",
    "                min_step=None,\n",
    "                max_step=None,\n",
    "                clip_coef=0,\n",
    "                use_stable_dream_loss=True,\n",
    "                 output_folder_name='peekaboo_results',\n",
    "                )->PeekabooResults:\n",
    "    \n",
    "    s=sd._get_stable_diffusion_singleton()\n",
    "    \n",
    "    if label is None: \n",
    "        label=SimpleLabel(name)\n",
    "    \n",
    "    image_path='<No image path given>'\n",
    "    if isinstance(image,str):\n",
    "        image_path=image\n",
    "        image=rp.load_image(image)\n",
    "    \n",
    "    assert rp.is_image(image)\n",
    "    assert issubclass(type(label),BaseLabel)\n",
    "    image=rp.as_rgb_image(rp.as_float_image(make_image_square(image,square_image_method)))\n",
    "    rp.tic()\n",
    "    time_started=rp.get_current_date()\n",
    "    \n",
    "    \n",
    "    log_cell('Get Hyperparameters') ########################################################################\n",
    "    icecream.ic(GRAVITY, BATCH_SIZE, NUM_ITER, LEARNING_RATE, GUIDANCE_SCALE,  representation, bilateral_kwargs, square_image_method)\n",
    "\n",
    "\n",
    "\n",
    "    # log_cell('Alpha Initializer') ########################################################################\n",
    "\n",
    "    p=PeekabooSegmenter(image,\n",
    "                        labels=[label],\n",
    "                        name=name,\n",
    "                        bilateral_kwargs=bilateral_kwargs,\n",
    "                        representation=representation, \n",
    "                        min_step=min_step,\n",
    "                        max_step=max_step,\n",
    "                       ).to(s.device)\n",
    "\n",
    "    if 'bilateral' in representation:\n",
    "        blur_image=rp.as_numpy_image(p.alphas.bilateral_blur(p.foreground))\n",
    "        print(\"The bilateral blur applied to the input image before/after, to visualize it\")\n",
    "        rp.display_image(rp.tiled_images(rp.labeled_images([rp.as_numpy_image(p.foreground),blur_image],['before','after'])))\n",
    "\n",
    "    p.display();\n",
    "\n",
    "\n",
    "    \n",
    "    # log_cell('Create Optimizers') ########################################################################\n",
    "\n",
    "    params=list(p.parameters())\n",
    "    optim=torch.optim.Adam(params,lr=1e-3)\n",
    "    optim=torch.optim.SGD(params,lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "    # log_cell('Create Logs') ########################################################################\n",
    "    global iter_num\n",
    "    iter_num=0\n",
    "    timelapse_frames=[]\n",
    "\n",
    "\n",
    "    # log_cell('Do Training') ########################################################################\n",
    "    preview_interval=NUM_ITER//10 #Show 10 preview images throughout training to prevent output from being truncated\n",
    "    preview_interval=1 #I want EVERY FRAME!!!\n",
    "    preview_interval=max(1,preview_interval)\n",
    "    log(\"Will show preview images every %i iterations\"%(preview_interval))\n",
    "\n",
    "    try:\n",
    "        display_eta=rp.eta(NUM_ITER)\n",
    "        for _ in range(NUM_ITER):\n",
    "            display_eta(_)\n",
    "            iter_num+=1\n",
    "\n",
    "            alphas=p.alphas()\n",
    "\n",
    "            for __ in range(BATCH_SIZE):\n",
    "                p.randomize_background()\n",
    "                composites=p()\n",
    "                for label, composite in zip(p.labels, composites):\n",
    "                    if clip_coef>0: \n",
    "                        #Use clip instead of stable-dream-loss\n",
    "                        #You must use 'name' for the prompt in this case\n",
    "                        from .clip import get_clip_logits\n",
    "                        logit=get_clip_logits(composite, label.name)*clip_coef\n",
    "                        loss=-logit\n",
    "                        loss.sum().backward(retain_graph=True)\n",
    "                        print(float(loss.sum()))\n",
    "                    if use_stable_dream_loss:\n",
    "                        s.train_step(label.embedding, composite[None], \n",
    "                                     guidance_scale=GUIDANCE_SCALE\n",
    "                                    )\n",
    "                        \n",
    "\n",
    "            ((alphas.sum())*GRAVITY).backward(retain_graph=True)\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if not _%100:\n",
    "                    # Don't overflow the notebook\n",
    "                    clear_output()\n",
    "                if not _%preview_interval: \n",
    "                    timelapse_frames.append(p.display())\n",
    "                    # rp.ptoc()\n",
    "    except KeyboardInterrupt:\n",
    "        log(\"Interrupted early, returning current results...\")\n",
    "        pass\n",
    "\n",
    "    output_folder = rp.make_folder('%s/%s'%(output_folder_name,name))\n",
    "    output_folder += '/%03i'%len(rp.get_subfolders(output_folder))\n",
    "    \n",
    "                \n",
    "    # rp.ptoc()\n",
    "    results = PeekabooResults(\n",
    "        #The main output is the alphas\n",
    "        alphas=rp.as_numpy_array(alphas),\n",
    "        \n",
    "        #Keep track of hyperparameters used\n",
    "        GRAVITY=GRAVITY,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        NUM_ITER=NUM_ITER,\n",
    "        GUIDANCE_SCALE=GUIDANCE_SCALE,\n",
    "        LEARNING_RATE=LEARNING_RATE,\n",
    "        bilateral_kwargs=bilateral_kwargs,\n",
    "        representation=representation,\n",
    "        \n",
    "        #Keep track of the inputs used\n",
    "        label=label,\n",
    "        image=image,\n",
    "        image_path=image_path,\n",
    "        clip_coef=clip_coef,\n",
    "        \n",
    "        use_stable_dream_loss=use_stable_dream_loss,\n",
    "        #Record some extra info\n",
    "        preview_image=p.display(),\n",
    "        timelapse_frames=rp.as_numpy_array(timelapse_frames),\n",
    "        **({'blur_image':blur_image} if 'blur_image' in dir() else {}),\n",
    "        height=p.height,\n",
    "        width=p.width,\n",
    "        p_name=p.name,\n",
    "        output_folder=rp.get_absolute_path(output_folder),\n",
    "        \n",
    "        min_step=p.min_step,\n",
    "        max_step=p.max_step,\n",
    "        \n",
    "        git_hash=rp.get_current_git_hash(), \n",
    "        time_started=rp.r._format_datetime(time_started),\n",
    "        time_completed=rp.r._format_datetime(rp.get_current_date()),\n",
    "        device=s.device,\n",
    "        computer_name=rp.get_computer_name(),\n",
    "    ) \n",
    "\n",
    "    save_peekaboo_results(results,output_folder)\n",
    "    print(\"Please wait - creating a training timelapse\")\n",
    "    \n",
    "    clear_output()\n",
    "    rp.display_image_slideshow(timelapse_frames)#This can take a bit of time\n",
    "    print(\"Saved results at %s\"%output_folder)\n",
    "    icecream.ic(name,label,image_path, GRAVITY, BATCH_SIZE, NUM_ITER, GUIDANCE_SCALE,  bilateral_kwargs)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Sgqq2J32Qzs",
    "outputId": "eee9073d-e09f-49ca-ca1a-5bb8b8d5e19c"
   },
   "outputs": [],
   "source": [
    "experiment_setting_presets = [\n",
    "    # dict(\n",
    "    #     representation=\"raster bilateral\",\n",
    "    #     LEARNING_RATE=1e-0,\n",
    "    #     # NUM_ITER=500,\n",
    "    #     GRAVITY=0.05,\n",
    "    #     clip_coef=500,\n",
    "    #     use_stable_dream_loss=False,\n",
    "    # ),\n",
    "    dict( #This one is also good!\n",
    "        representation=\"raster bilateral\",\n",
    "        LEARNING_RATE=1e-0,\n",
    "        GUIDANCE_SCALE=300,\n",
    "        NUM_ITER=500,\n",
    "        GRAVITY=0.05,\n",
    "        # min_step=10,\n",
    "        # max_step=600,\n",
    "    ),\n",
    "    dict(represntation=\"fourier\",GRAVITY=0,NUM_ITER=300),\n",
    "dict(representation=\"fourier\"),\n",
    "\n",
    "    # dict(representation=\"fourier\"),\n",
    "    # dict(),\n",
    "    dict(representation=\"raster\", LEARNING_RATE=1, GUIDANCE_SCALE=200,GRAVITY=.1),\n",
    "    \n",
    "    # dict(\n",
    "    #     representation=\"raster bilateral\",\n",
    "    #     LEARNING_RATE=1e-0,\n",
    "    #     GUIDANCE_SCALE=200,\n",
    "    #     # NUM_ITER=500,\n",
    "    #     GRAVITY=0.05,\n",
    "    #     min_step=200,\n",
    "    #     max_step=400,\n",
    "    # ),\n",
    "    # dict(\n",
    "    #     representation=\"raster bilateral\",\n",
    "    #     LEARNING_RATE=1e-0,\n",
    "    #     # NUM_ITER=500,\n",
    "    #     GRAVITY=0.05,\n",
    "    #     clip_coef=500,\n",
    "    #     use_stable_dream_loss=True,\n",
    "    #     GUIDANCE_SCALE=200,\n",
    "    #     min_step=10,\n",
    "    #     max_step=600,\n",
    "    # ),\n",
    "    # dict(NUM_ITER=500),\n",
    "]\n",
    "\n",
    "\n",
    "# experiment_setting_presets=[dict()]\n",
    "\n",
    "things_to_try=[]\n",
    "\n",
    "for prompts, urls in rp.shuffled(experiment_input_data):\n",
    "    for prompt in prompts:\n",
    "        for url in urls:\n",
    "            things_to_try.append([prompt,url])\n",
    "\n",
    "i=0\n",
    "while True:\n",
    "    # things_to_try=rp.shuffled(things_to_try)\n",
    "    for prompt,url in things_to_try:\n",
    "        \n",
    "        for preset in experiment_setting_presets:\n",
    "            i+=1\n",
    "            print('EXPERIMENT NUMBER:',i)\n",
    "\n",
    "            rp.ic(prompt,url,preset)\n",
    "\n",
    "            results = run_peekaboo(\n",
    "                prompt,\n",
    "                url,\n",
    "                **preset,\n",
    "                output_folder_name='peekaboo_for_RPE'\n",
    "            )\n",
    "\n",
    "            get_result_thumbnail(results)\n",
    "        from IPython.display import clear_output\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OTHER STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Sgqq2J32Qzs",
    "outputId": "eee9073d-e09f-49ca-ca1a-5bb8b8d5e19c"
   },
   "outputs": [],
   "source": [
    "results=run_peekaboo(\n",
    "    # 'Green Luigi',\n",
    "    # \"https://i1.sndcdn.com/artworks-000160550668-iwxjgo-t500x500.jpg\",\n",
    "\n",
    "\n",
    "    # 'orange basketball',\n",
    "    # 'brown american football',\n",
    "    # 'black and white soccer ball on the bottom right',\n",
    "    # 'https://m.media-amazon.com/images/I/711pmt372VL.jpg',\n",
    "    \n",
    "    'Beverly Crusher',\n",
    "    # 'https://www.google.com/url?sa=i&url=https%3A%2F%2Fmemory-alpha.fandom.com%2Fwiki%2FBeverly_Crusher&psig=AOvVaw2_2z0ZVduCSQH1zpoNfPlY&ust=1675160497935000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCNDkx6qJ7_wCFQAAAAAdAAAAABAE',\n",
    "    'https://imgix.bustle.com/uploads/image/2020/6/30/5a0a61e2-5518-4bb9-b8ea-66e3b3986ad7-picard-cruhser.jpg?w=349&h=262&fit=max&auto=format%2Ccompress',\n",
    "    \n",
    "    # 'Harry Potter boy - daniel radcliffe - man with glasses',\n",
    "    # 'Hermoine Granger - Emma Watson - Girl with long hair',\n",
    "    # 'Hermoine',\n",
    "     # \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "     # \"https://i.guim.co.uk/img/media/380500f7fd9321a6cfd1bd1a3b3f104ad9797bb7/0_0_2000_1199/master/2000.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=f0d0ecb5a0c33aa91be270b08619e8d7\",\n",
    "    \n",
    "    # 'Dory the fish',\n",
    "    # 'https://akns-images.eonline.com/eol_images/Entire_Site/2016917/rs_600x600-161017135949-600.finding-dory-2.101716.jpg',\n",
    "\n",
    "    # 'brass shiny saxophone on top',\n",
    "     # \"https://c8.alamy.com/comp/2GFDH27/violin-with-saxophone-and-piano-closeup-2GFDH27.jpg\",\n",
    "\n",
    "    #Just CLIP\n",
    "    # representation='raster bilateral', LEARNING_RATE=1e-1, NUM_ITER=500,  GRAVITY=.15, clip_coef=5000, use_stable_dream_loss=False,\n",
    "    # representation='raster bilateral', LEARNING_RATE=1e-1, NUM_ITER=500,  GRAVITY=.05, clip_coef=3000, use_stable_dream_loss=False,\n",
    "    # representation='raster bilateral', LEARNING_RATE=1e-0, NUM_ITER=500,  GRAVITY=.05, clip_coef=500, use_stable_dream_loss=False, #BEST\n",
    "    \n",
    "    #Just DL\n",
    "     # representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05, min_step=10, max_step=600,\n",
    "\n",
    "    #Both\n",
    "    representation='raster bilateral', LEARNING_RATE=1e-0, NUM_ITER=500,  GRAVITY=.05, clip_coef=500, use_stable_dream_loss=True, GUIDANCE_SCALE=200, min_step=10, max_step=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Sgqq2J32Qzs",
    "outputId": "eee9073d-e09f-49ca-ca1a-5bb8b8d5e19c"
   },
   "outputs": [],
   "source": [
    "#Mario vs Luigi Part 1\n",
    "# peekaboo.s.min_step, peekaboo.s.max_step=200,600\n",
    "results=run_peekaboo(\n",
    "    # 'Green Luigi',\n",
    "    # \"https://i1.sndcdn.com/artworks-000160550668-iwxjgo-t500x500.jpg\",\n",
    "\n",
    "\n",
    "    # 'orange basketball',\n",
    "    # 'brown american football',\n",
    "    'black and white soccer ball on the bottom right',\n",
    "    'https://m.media-amazon.com/images/I/711pmt372VL.jpg',\n",
    "    \n",
    "    # 'Harry Potter boy - daniel radcliffe - man with glasses',\n",
    "    # 'Hermoine Granger - Emma Watson - Girl with long hair',\n",
    "    # 'Hermoine',\n",
    "     # \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "     # \"https://i.guim.co.uk/img/media/380500f7fd9321a6cfd1bd1a3b3f104ad9797bb7/0_0_2000_1199/master/2000.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=f0d0ecb5a0c33aa91be270b08619e8d7\",\n",
    "    \n",
    "    # 'Dory the fish',\n",
    "    # 'https://akns-images.eonline.com/eol_images/Entire_Site/2016917/rs_600x600-161017135949-600.finding-dory-2.101716.jpg',\n",
    "\n",
    "    # 'brass shiny saxophone on top',\n",
    "     # \"https://c8.alamy.com/comp/2GFDH27/violin-with-saxophone-and-piano-closeup-2GFDH27.jpg\",\n",
    "\n",
    "    #Just CLIP\n",
    "    # representation='raster bilateral', LEARNING_RATE=1e-1, NUM_ITER=500,  GRAVITY=.15, clip_coef=5000, use_stable_dream_loss=False,\n",
    "    # representation='raster bilateral', LEARNING_RATE=1e-1, NUM_ITER=500,  GRAVITY=.05, clip_coef=3000, use_stable_dream_loss=False,\n",
    "    # representation='raster bilateral', LEARNING_RATE=1e-0, NUM_ITER=500,  GRAVITY=.05, clip_coef=500, use_stable_dream_loss=False, #BEST\n",
    "    \n",
    "    #Just DL\n",
    "     # representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05, min_step=10, max_step=600,\n",
    "\n",
    "    #Both\n",
    "    representation='raster bilateral', LEARNING_RATE=1e-0, NUM_ITER=500,  GRAVITY=.05, clip_coef=500, use_stable_dream_loss=True, GUIDANCE_SCALE=200, min_step=10, max_step=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Sgqq2J32Qzs",
    "outputId": "eee9073d-e09f-49ca-ca1a-5bb8b8d5e19c"
   },
   "outputs": [],
   "source": [
    "peekaboo.s.min_step=200\n",
    "peekaboo.s.max_step=600\n",
    "\n",
    "#Mario vs Luigi Part 1\n",
    "results=run_peekaboo(\n",
    "    # 'Luigi', #200/600\n",
    "    # 'Princess Peach', #200/600H, \n",
    "    # 'Mario',\n",
    "    # 'Bowser from super smash bros; yellow shell; seen from the front', #NO; stable diffusion is too weak to draw this - or I have bad prompts\n",
    "    \n",
    "    \n",
    "                     # \"https://i1.sndcdn.com/artworks-000160550668-iwxjgo-t500x500.jpg\",\n",
    "                     # \"http://images6.fanpop.com/image/photos/43400000/Mario-Peach-super-mario-bros-43431011-644-704.png\",\n",
    "                                         \n",
    "                     # bilateral_kwargs=dict(kernel_size = 3,\n",
    "                     #                  tolerance = .08,\n",
    "                     #                  sigma = 5,\n",
    "                     #                  iterations=2,\n",
    "                     #                 ),\n",
    "                     # representation='raster',\n",
    "    \n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Sgqq2J32Qzs",
    "outputId": "eee9073d-e09f-49ca-ca1a-5bb8b8d5e19c"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    results=run_peekaboo('Luigi',\n",
    "                         \"https://i1.sndcdn.com/artworks-000160550668-iwxjgo-t500x500.jpg\",\n",
    "                         representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, GRAVITY=.05,\n",
    "                         min_step=200,\n",
    "                         max_step=400,\n",
    "                         NUM_ITER=100,\n",
    "                         )\n",
    "    results_collection.append(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mario vs Luigi Part 2\n",
    "results=run_peekaboo('Luigi',\n",
    "                     \"https://i1.sndcdn.com/artworks-000160550668-iwxjgo-t500x500.jpg\",\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QWGA9wjKPp6S",
    "outputId": "b9a1bde6-08b7-4336-a98b-9eda0bc762ea"
   },
   "outputs": [],
   "source": [
    "#Mario vs Luigi Part 2\n",
    "label = peekaboo.SimpleLabel('Luigi')\n",
    "blend_factor=.5\n",
    "label.embedding = label.embedding*(1+blend_factor) - peekaboo.SimpleLabel('Mario').embedding*blend_factor\n",
    "results=run_peekaboo('Luigi',\n",
    "                     \"https://i1.sndcdn.com/artworks-000160550668-iwxjgo-t500x500.jpg\",\n",
    "                     label=label\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WsaoICr3PskD",
    "outputId": "e08dccb8-390b-489e-dcfc-d022c4f27f54"
   },
   "outputs": [],
   "source": [
    "peekaboo.s.min_step,peekaboo.s.max_step=200,400\n",
    "#Wine and Cheese Part 1\n",
    "results=run_peekaboo('Wine Glass half-full',\n",
    "                     \"https://emeraldvalleyartisans.com/wp-content/uploads/2015/10/winecheese1.jpg\",\n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WsaoICr3PskD",
    "outputId": "e08dccb8-390b-489e-dcfc-d022c4f27f54"
   },
   "outputs": [],
   "source": [
    "peekaboo.s.min_step,peekaboo.s.max_step=200,600\n",
    "#Wine and Cheese Part 1\n",
    "results=run_peekaboo('Wine Glass half-full',\n",
    "                     'Swiss Cheese',\n",
    "                     'Grapes',\n",
    "                     'Green Grapes',\n",
    "                     \"https://emeraldvalleyartisans.com/wp-content/uploads/2015/10/winecheese1.jpg\",\n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peekaboo.s.min_step,peekaboo.s.max_step=200,400\n",
    "#Wine and Cheese Part 2\n",
    "results=run_peekaboo('Swiss Cheese',\n",
    "                     \"https://emeraldvalleyartisans.com/wp-content/uploads/2015/10/winecheese1.jpg\",\n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peekaboo.s.min_step,peekaboo.s.max_step=200,600\n",
    "#Wine and Cheese Part 2\n",
    "results=run_peekaboo('Swiss Cheese',\n",
    "                     \"https://emeraldvalleyartisans.com/wp-content/uploads/2015/10/winecheese1.jpg\",\n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://americasrestaurant.com/wp-content/uploads/2022/04/what-do-bread-and-butter-pickles-taste-like.jpg\n",
    "peekaboo.s.min_step,peekaboo.s.max_step=200,600\n",
    "#Wine and Cheese Part 3\n",
    "results=run_peekaboo(\n",
    "    # 'piano keyboard',\n",
    "    # 'violin',\n",
    "    'brass shiny saxophone on top',\n",
    "                     \"https://c8.alamy.com/comp/2GFDH27/violin-with-saxophone-and-piano-closeup-2GFDH27.jpg\",\n",
    "                     # representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    results=run_peekaboo(\n",
    "        'Nemo Clownfish',\n",
    "        'https://akns-images.eonline.com/eol_images/Entire_Site/2016917/rs_600x600-161017135949-600.finding-dory-2.101716.jpg',\n",
    "        representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200,\n",
    "        min_step=200, max_step=600,\n",
    "        GRAVITY=.05,\n",
    "        NUM_ITER=200,\n",
    "    )\n",
    "    results_collection.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    results=run_peekaboo(\n",
    "        'brass shiny saxophone on top',\n",
    "        \"https://c8.alamy.com/comp/2GFDH27/violin-with-saxophone-and-piano-closeup-2GFDH27.jpg\",\n",
    "        representation='raster bilateral',\n",
    "        LEARNING_RATE=1e-0,\n",
    "        GUIDANCE_SCALE=200,\n",
    "        min_step=200,\n",
    "        max_step=900,\n",
    "        GRAVITY=.05,\n",
    "        NUM_ITER=200,\n",
    "    )\n",
    "    results_collection.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peekaboo.s.min_step,peekaboo.s.max_step=200,400\n",
    "#Wine and Cheese Part 3\n",
    "results=run_ipeekaboo('grapes',\n",
    "                     \"https://emeraldvalleyartisans.com/wp-content/uploads/2015/10/winecheese1.jpg\",\n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peekaboo.s.min_step,peekaboo.s.max_step=200,600\n",
    "#Wine and Cheese Part 3\n",
    "results=run_peekaboo('Green grapes',\n",
    "                     \"https://emeraldvalleyartisans.com/wp-content/uploads/2015/10/winecheese1.jpg\",\n",
    "                     representation='raster bilateral', LEARNING_RATE=1e-0, GUIDANCE_SCALE=200, NUM_ITER=500,  GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aED-B2o-Wi8y",
    "outputId": "cce88287-a40e-4b2d-a20d-c3faa0a7fa64"
   },
   "outputs": [],
   "source": [
    "#Harry Potter Part 1\n",
    "results=run_peekaboo('Harry Potter boy - daniel radcliffe - man with glasses',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     NUM_ITER=300\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harry Potter Part 2\n",
    "label = peekaboo.SimpleLabel('Hermoine Granger')\n",
    "blend_factor=.5\n",
    "label.embedding = label.embedding*(1+blend_factor) - peekaboo.SimpleLabel('Harry Potter').embedding*blend_factor\n",
    "results=run_peekaboo('Hermoine',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     label=label\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harry Potter Part 2\n",
    "label = peekaboo.SimpleLabel('Hermoine Granger - Emma Watson - Girl with long hair')\n",
    "blend_factor=.5\n",
    "label.embedding = label.embedding*(1+blend_factor) - peekaboo.SimpleLabel('Harry Potter boy - daniel radcliffe - man with glasses').embedding*blend_factor\n",
    "\n",
    "results=run_peekaboo('Hermoine',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     label=label\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Emma Watson\n",
    "#Good in a row: |||\n",
    "#Bad prompts: \"Hermoine Granger aka Emma Watson\"\n",
    "results=run_peekaboo('Hermoine Granger - Emma Watson - Girl with long hair',\n",
    "                     \"https://cometothedarkside97.files.wordpress.com/2021/05/images-2020-01-25t073939.551.jpeg?w=468\",\n",
    "                     representation='raster bilateral',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emma Watson\n",
    "#Good in a row: \n",
    "#Bad prompts: \n",
    "results=run_peekaboo('Harry Potter boy - daniel radcliffe - man with glasses',\n",
    "                     \"https://prisiliaandayani.files.wordpress.com/2014/03/harry-ron-hermione-harry-potter-2422078-361-500.jpg\",\n",
    "                     representation='raster bilateral',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good in a row with fixed timestep 500: H,H,\n",
    "#Bad prompts: \n",
    "\n",
    "# peekaboo.s.min_step=100\n",
    "# peekaboo.s.max_step=500\n",
    "for _ in range(10):\n",
    "    results=run_peekaboo(\n",
    "        'Harry Potter boy - daniel radcliffe - man with glasses',\n",
    "        # 'Hermoine Granger - Emma Watson - Girl with long hair',\n",
    "\n",
    "                         \"https://i.guim.co.uk/img/media/380500f7fd9321a6cfd1bd1a3b3f104ad9797bb7/0_0_2000_1199/master/2000.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=f0d0ecb5a0c33aa91be270b08619e8d7\",\n",
    "                            # \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                         # \"https://prisiliaandayani.files.wordpress.com/2014/03/harry-ron-hermione-harry-potter-2422078-361-500.jpg\",\n",
    "\n",
    "                         representation='raster bilateral',\n",
    "                         # representation='raster',\n",
    "                         # bilateral_kwargs=dict(kernel_size = 3,\n",
    "                         #                  tolerance = .08,\n",
    "                         #                  sigma = 5,\n",
    "                         #                  iterations=2,\n",
    "                         #                 ),\n",
    "                         # representation='raster',\n",
    "                         LEARNING_RATE=1e-0,\n",
    "                         GUIDANCE_SCALE=200,\n",
    "                         min_step=200,\n",
    "                         max_step=400,\n",
    "                         GRAVITY=.05,\n",
    "                         NUM_ITER=200,\n",
    "                         )\n",
    "    results_collection.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emma Watson\n",
    "#Good in a row: ||X2X\n",
    "#Good in a row with fixed timestep: |||||\n",
    "#Bad prompts: \n",
    "results=run_peekaboo('Hermoine Granger - Emma Watson - Girl with long hair',\n",
    "                     \"https://i.guim.co.uk/img/media/380500f7fd9321a6cfd1bd1a3b3f104ad9797bb7/0_0_2000_1199/master/2000.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=f0d0ecb5a0c33aa91be270b08619e8d7\",\n",
    "                     representation='raster bilateral',\n",
    "                     # bilateral_kwargs=dict(kernel_size = 3,\n",
    "                     #                  tolerance = .08,\n",
    "                     #                  sigma = 5,\n",
    "                     #                  iterations=2,\n",
    "                     #                 ),\n",
    "                     # representation='raster',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emma Watson\n",
    "#Good in a row: |||\n",
    "#Bad prompts: \"Emma Watson\"\n",
    "results=run_peekaboo('Hermoine Granger aka Emma Watson',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     representation='raster bilateral',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harry Potter \n",
    "#Good in a row: ||\n",
    "results=run_peekaboo('Daniel radcliffe',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     representation='raster bilateral',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harry Potter \n",
    "#Good in a row: |||||\n",
    "results=run_peekaboo('Harry Potter boy - daniel radcliffe - man with glasses',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     representation='raster bilateral',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mario vs Luigi Part 1\n",
    "#Good in a row: |||||\n",
    "results=run_peekaboo('Hermoine Granger - Emma Watson - Girl with long hair',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     representation='raster bilateral',\n",
    "                     LEARNING_RATE=1e-0,\n",
    "                     GRAVITY=.05,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xt0W5sRgXZzc",
    "outputId": "9ed0490b-6209-4188-96b0-45fe4892ae94"
   },
   "outputs": [],
   "source": [
    "#Harry Potter Part 2\n",
    "results=run_peekaboo('Hermoine Granger - Emma Watson - Girl with long hair - left',\n",
    "                     \"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",\n",
    "                     NUM_ITER=300\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "QcZYe07D92jA",
    "outputId": "7378697c-1ee0-49da-fc6c-db26df3ea552"
   },
   "outputs": [],
   "source": [
    "#Stable Diffusion Basic Demo\n",
    "#This is a demo cell showing how to simply use Stable Diffusion to generate an image. This is usefull for prompt engineering.\n",
    "rp.display_image(peekaboo.s.prompt_to_img('Cake'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "QcZYe07D92jA",
    "outputId": "7378697c-1ee0-49da-fc6c-db26df3ea552"
   },
   "outputs": [],
   "source": [
    "#Stable Diffusion Basic Demo\n",
    "#This is a demo cell showing how to simply use Stable Diffusion to generate an image. This is usefull for prompt engineering.\n",
    "for _ in range(30):\n",
    "    rp.display_image(peekaboo.s.prompt_to_img('Bowser from super smash bros; yellow shell; seen from the front'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.local_copy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10 (sd2_dream)",
   "language": "python",
   "name": "sd2_dream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
